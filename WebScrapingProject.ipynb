{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "\n",
    "While working on becoming a full time web developer, I decided take a part-time job working for a local non-profit. One of my tasks involved logging into a third party website, downloading a CSV file, formatting it in Excel and then printing it for use in a class that occurred twice a week.\n",
    "\n",
    "I soon realized that this was a perfect opportunity to put my skills as a programmer to use. So I set out to automate this task. Here are the steps I took to make this happen.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import used:\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "\n",
    "import requests\n",
    "\n",
    "from requests.auth import HTTPBasicAuth;\n",
    "\n",
    "from bs4 import BeautifulSoup;\n",
    "\n",
    "import re;\n",
    "\n",
    "import datetime\n",
    "\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import xlsxwriter\n",
    "\n",
    "import yagmail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step One: Get the class ID\n",
    "\n",
    "When first logging into the third party site with the data I needed, I was met with a table with columns that included the date of the class and its id. Clicking on the link for any particular class, I noted that the URL had two parameters: the date of the class and its ID. At first, I thought that simply passing the date might be enough. After all, there was only one class occurring on any particular date. I tested passing just the date and not the class id. I got back a table of data but it was the wrong data. So, as a first step in the project, I needed to get the id of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful\n"
     ]
    }
   ],
   "source": [
    "#Using a context manager, I open the first session to the third party website to get table\n",
    "#with class ids and dates: \n",
    "\n",
    "#Environment vars I need for this:\n",
    "site = os.getenv(\"ADMIN_SITE\")\n",
    "user = os.getenv(\"ADMIN_USER\")\n",
    "pswd = os.getenv(\"ADMIN_PSWD\")\n",
    "\n",
    "with requests.Session() as s:\n",
    "    resp = s.get(site, auth=HTTPBasicAuth(user, pswd), timeout=10)\n",
    "    \n",
    "    if resp.status_code == 200:\n",
    "        class_list = resp.content;\n",
    "        print(\"Connection successful\") #I add this line to show connection was successful.\n",
    "        \n",
    "    else:\n",
    "        print(\"Connection could not be establised. Status code: {}\".format(resp.status_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Get class ids and dates\n",
    "\n",
    "Having successfully connected to the site, I now had access to the HTML that included the table with class dates and ids. As a next step, I scraped that contents of the table and placed them in a Python dictionary for later use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'623': '2019-01-01', '599': '2019-06-01', '600': '2019-06-02', '601': '2019-06-03', '602': '2019-06-04', '603': '2019-06-08', '604': '2019-06-09', '605': '2019-06-10', '606': '2019-06-11', '607': '2019-06-15', '608': '2019-06-16', '609': '2019-06-17', '610': '2019-06-18', '611': '2019-06-22', '612': '2019-06-23', '613': '2019-06-24', '614': '2019-06-25', '615': '2019-06-29', '616': '2019-06-30', '617': '2019-07-01', '618': '2019-07-02', '619': '2019-07-06', '620': '2019-07-07', '621': '2019-07-08', '622': '2019-07-09', '624': '2019-07-13', '625': '2019-07-14', '626': '2019-07-15', '627': '2019-07-16', '628': '2019-07-20', '629': '2019-07-21', '630': '2019-07-22', '632': '2019-07-23', '633': '2019-07-27', '634': '2019-07-28', '635': '2019-07-29', '636': '2019-07-30', '637': '2019-08-03', '638': '2019-08-04', '639': '2019-08-05', '640': '2019-08-06', '641': '2019-08-10', '642': '2019-08-11', '643': '2019-08-12', '644': '2019-08-13', '645': '2019-08-17', '646': '2019-08-18', '647': '2019-08-19', '648': '2019-08-20', '649': '2019-08-24', '650': '2019-08-25', '651': '2019-08-26', '652': '2019-08-27', '653': '2019-08-31', '654': '2019-09-01', '655': '2019-09-03', '656': '2019-09-07', '666': '2019-09-08', '672': '2019-09-08', '658': '2019-09-09', '659': '2019-09-10', '660': '2019-09-14', '664': '2019-09-15', '662': '2019-09-16', '663': '2019-09-17', '667': '2019-09-21', '668': '2019-09-22', '671': '2019-09-23', '670': '2019-09-24', '673': '2019-09-28', '674': '2019-09-29', '675': '2019-09-30', '676': '2019-10-01', '677': '2019-10-05', '678': '2019-10-06', '679': '2019-10-07', '680': '2019-10-08', '681': '2019-10-12', '682': '2019-10-13', '683': '2019-10-14', '684': '2019-10-15', '685': '2019-10-19', '686': '2019-10-20', '687': '2019-10-21', '688': '2019-10-22', '689': '2019-10-26', '690': '2019-10-27', '691': '2019-10-28', '692': '2019-10-29', '693': '2019-11-02', '694': '2019-11-03', '695': '2019-11-04', '696': '2019-11-05', '697': '2019-11-09', '698': '2019-11-10', '699': '2019-11-11', '700': '2019-11-12', '702': '2019-11-16', '704': '2019-11-17', '705': '2019-11-18', '707': '2019-11-19', '708': '2019-11-23', '710': '2019-11-24', '711': '2019-11-25', '712': '2019-11-26', '713': '2019-11-30', '714': '2019-12-01', '715': '2019-12-02', '716': '2019-12-03'}\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(class_list, 'html.parser')\n",
    "\n",
    "links = soup.find_all('a')\n",
    "\n",
    "#The links I parsed were returned with the class id and date as attributes. So I used regular \n",
    "#expressions to pull out the content I needed with the following regex:\n",
    "\n",
    "id_regex = re.compile(\"(?<=class_id=)\\d{3,}\")\n",
    "\n",
    "date_regex = re.compile(\"(?<=class_date=)\\d{4}-\\d{2}-\\d{2}\")\n",
    "\n",
    "#Empty lists to hold id and class date data:\n",
    "\n",
    "ids = []\n",
    "\n",
    "dates = []\n",
    "\n",
    "#Loop through links to get data I need and append the result to the lists above:\n",
    "\n",
    "\n",
    "for item in links:\n",
    "\n",
    "    id_matches = re.findall(id_regex, str(item))\n",
    "\n",
    "    date_matches = re.findall(date_regex, str(item))\n",
    "\n",
    "    if len(id_matches) > 0:\n",
    "\n",
    "        ids.append(id_matches[0])\n",
    "\n",
    "    if len(date_matches) > 0:\n",
    "\n",
    "        dates.append(date_matches[0])\n",
    "\n",
    "#Finally, I zip the id and dates into a dictionary.\n",
    "dateId = dict(zip(ids, dates))\n",
    "\n",
    "\n",
    "#Here is the dateId data that resulted from all this:\n",
    "\n",
    "print(dateId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Is there a class today?\n",
    "\n",
    "Now that I have the class dates and their respective ids, I could now test if there was a class on any particular date and, if there was, gather the data I would need for my excel spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First, I get today's date:\n",
    "\n",
    "today = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "#Next I check if there is a date in the dictionary above that matches today's date. If so, I\n",
    "#grab the class date and id:\n",
    "\n",
    "for k, v in dateId.items():\n",
    "\n",
    "    if v == today:\n",
    "\n",
    "        classId = k;\n",
    "\n",
    "        classDate = v;     \n",
    "        \n",
    "#Using an environment variable to get part of the url I'll need:\n",
    "classLink = os.getenv(\"CLASS_LINK\")\n",
    "\n",
    "#Now if there is a class date that matches today's date, I use it to create the url I'll need\n",
    "#for the 2nd HTTP request:\n",
    "\n",
    "if classDate == today:\n",
    "\n",
    "    newUrl = \"{}class_id={}&class_date={}\".format(classLink, classId, classDate)\n",
    "    \n",
    "else:\n",
    "\n",
    "    print(\"No class today\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Next I open a new HTTP request using the newUrl created because we did not get a message\n",
    "#saying there was no class today:\n",
    "\n",
    "if len(newUrl) != 0:\n",
    "\n",
    "    with requests.Session() as s2:\n",
    "\n",
    "        resp2 = s2.get(newUrl,auth=HTTPBasicAuth(user, pswd), timeout=10)\n",
    "        \n",
    "#Now I parse the resp2 result:\n",
    "\n",
    "table_soup = BeautifulSoup(resp2.content, \"html.parser\")\n",
    "\n",
    "#And extract the table data, which I found located in a table with the class of \"body-text\"\n",
    "\n",
    "class_register = table_soup.find(\"table\", {\"class\" : \"bodytext\"} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Extract data from HTML table and initial formatting\n",
    "\n",
    "In this next step, I want to extract the data I need from the HTML tags. Going forward, I will \n",
    "ask the reader to assume that I am succeeding in these steps as I do not want to publish data without permission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pull out all table rows:\n",
    "\n",
    "table_rows = class_register.find_all('tr')\n",
    "\n",
    "#creating an empty data list to hold the data:\n",
    "\n",
    "data = [];\n",
    "\n",
    "#looping through the table rows to get the data and populate it into the data array:\n",
    "\n",
    "for tr in table_rows:\n",
    "\n",
    "    td = tr.find_all('td')\n",
    "\n",
    "    row = [i.text for i in td]\n",
    "\n",
    "    data.append(row)\n",
    "    \n",
    "#to make it easier to visualize and work with the resulting data, I used pandas:\n",
    "\n",
    "dat = pd.DataFrame(data[1:], columns=data[0])\n",
    "\n",
    "#Removing some unneeded columns:\n",
    "\n",
    "dat.drop(['Request Date', 'SSN', 'Attended', 'DL Number', \n",
    "          'TLC Number', 'Car Number', 'Class ID'], axis=1, inplace=True)\n",
    "\n",
    "#There was one more column that I wanted to remove:\n",
    "\n",
    "d = dat.iloc[:, 1:]\n",
    "\n",
    "#I need an empty Signature column to gather student signatures later\n",
    "\n",
    "blank = pd.DataFrame(columns=['Signature'])\n",
    "\n",
    "d = pd.concat([d, blank], axis=1)\n",
    "\n",
    "d.fillna(\" \", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Complete formatting of the data and store to excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Helper function to remove whitespaces and capitalize certain columns:\n",
    "\n",
    "def stripAndCap(col):\n",
    "\n",
    "    d[col] = d[col].str.strip().str.capitalize()\n",
    "    \n",
    "cap_cols = ['First Name', 'Last Name', 'City',  'Company Base']\n",
    "\n",
    "[stripAndCap(i) for i in cap_cols]\n",
    "\n",
    "#Now a series of data formatting with the help of pandas, list comprehensions, regex, etc.\n",
    "\n",
    "d['State'] = d['State'].str.strip().str.upper()\n",
    "d['Email'] = d['Email'].str.strip().str.lower()\n",
    "d['Cell Number'] = d['Cell Number'].str.replace(r'[^0-9]', '').str.replace(r'^1', '')\n",
    "d['Cell Number'] = [\"-\".join(re.match(\"(\\d{3})(\\d{3})(\\d{4})\", i).groups()) \n",
    "                    for i in d['Cell Number']]\n",
    "d['Company Base'] = d['Company Base'].str.strip().str.replace(r'(?<=\\w)[-|,]\\S+', '')\n",
    "\n",
    "for i in d['Address'].str.strip():\n",
    "\n",
    "    for j in i.split():\n",
    "\n",
    "        ''.join(j.capitalize())\n",
    "\n",
    "d['Address'] = [' '.join([j.capitalize() for j in i.split()]) for i in d['Address'].str.strip()]\n",
    "\n",
    "#Sort the values on Last Name variable:\n",
    "\n",
    "d.sort_values(by=['Last Name'], inplace=True)\n",
    "\n",
    "#Realign columns in order I need them:\n",
    "\n",
    "d = d[['Last Name', 'First Name', 'Address', 'City', 'State', 'Zip',\n",
    "\n",
    "         'Company Base', 'Cell Number', 'Email', 'Signature']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Importing data into excel and prepping data for printing\n",
    "\n",
    "In this step, I make use of the xlsxwriter package to prepare the data to be print ready for the end user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Note the name of the written file: \"DDC_[today's date].xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(f'DDC_{today}.xlsx', engine='xlsxwriter') as writer:\n",
    "\n",
    "    d.to_excel(writer, header=True, index=False)\n",
    "    \n",
    "    workbook = writer.book\n",
    "\n",
    "    worksheet = writer.sheets['Sheet1']\n",
    "    \n",
    "    worksheet.set_header('&C&24&\"Bold\"Class Register &D')\n",
    "    \n",
    "    worksheet.set_default_row(60, hide_unused_rows=True)\n",
    "    \n",
    "    #Setting the formats of each of the column headers.\n",
    "    \n",
    "    header_format = workbook.add_format({\n",
    "\n",
    "                                            'bold': 1,\n",
    "\n",
    "                                            'align': 'center',\n",
    "\n",
    "                                            'font_size': 16,\n",
    "\n",
    "                                            'valign': 'vcenter',\n",
    "\n",
    "                                            'bg_color': \"#F8F8F8\",\n",
    "\n",
    "                                            'bottom' : 6,\n",
    "\n",
    "                                            'left': 1,\n",
    "\n",
    "                                            'right': 1\n",
    "\n",
    "                                            })\n",
    "    \n",
    "    for col, val in enumerate(d.columns):\n",
    "\n",
    "        worksheet.write(0, col, val, header_format)\n",
    "        \n",
    "    #Adding some default column formatting\n",
    "        \n",
    "    defaultColFormat = workbook.add_format({'font_size': 15,\n",
    "\n",
    "                                                'align': 'center',\n",
    "\n",
    "                                                'text_wrap': 1\n",
    "\n",
    "                                                })\n",
    "\n",
    "\n",
    "\n",
    "    worksheet.set_column(0, 10, 16, defaultColFormat)\n",
    "    \n",
    "    #Adjusting the widths of certain columns:\n",
    "    \n",
    "    worksheet.set_column('C:C', 36)\n",
    "\n",
    "    worksheet.set_column('D:D', 14)\n",
    "\n",
    "    worksheet.set_column('E:E', 6)\n",
    "\n",
    "    worksheet.set_column('F:F', 8)\n",
    "\n",
    "    worksheet.set_column('G:G', 18)\n",
    "\n",
    "    worksheet.set_column('I:I', 36)\n",
    "\n",
    "    worksheet.set_column('J:J', 48)\n",
    "    \n",
    "    #Setting page layout and printing options:\n",
    "    \n",
    "    \n",
    "    worksheet.set_landscape()\n",
    "\n",
    "    worksheet.center_horizontally()\n",
    "\n",
    "    worksheet.set_margins(left=0, right=0, top=0.6, bottom=0.1)\n",
    "\n",
    "    worksheet.hide_gridlines(0)\n",
    "\n",
    "    worksheet.fit_to_pages(1, 2)\n",
    "\n",
    "    worksheet.repeat_rows(0)\n",
    "\n",
    "    #Finally, I save the 'writer' context manager\n",
    "    \n",
    "    writer.save()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDC.py                    WebScrapingProject.ipynb\r\n",
      "DDC_2019-11-11.xlsx       oauth2.json\r\n"
     ]
    }
   ],
   "source": [
    "#Note that the DDC excel file has been successfully created in the project directory \n",
    "#(DDC_2019-11-11.xlsx):\n",
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Setting up the emailing of the excel file to office admin email\n",
    "\n",
    "For this step of the process, which I won't include the code for, I wanted to email the xlsx file created by my program to the office admin email. Here is a general overview of the steps I took in lieu of code:\n",
    "\n",
    "- Imported yagmail\n",
    "- Visited google developer link to enable the gmail api.\n",
    "- In the gmail api, I created a new project and obtained the client id and secret.\n",
    "- I ran 'yag = yagmail.SMTP('my email', oauth2_file=\"oauth2.json\"). The oauth2 file was left blank at first.\n",
    "- Running the above code, I was prompted to enter my email, client id and client secret. This then led to a link I had to visit in gmail to get a token. I then copied and pasted the token into the final prompt. At this point, I was able to connect to gmail API with OAuth2.\n",
    "- After successfully testing this process, I set my contents variable equal to the name of the xlsx file. It is important to include the xlsx part of the file to ensure correct processing.\n",
    "- Then, I ran yag.send(to=[the admin email], contents = contents)\n",
    "\n",
    "The above steps resulted in a successful transmission of my excel file to the admin email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Automating the running of this code\n",
    "\n",
    "As a final step, I set up a bash script to run the python code described here. Then, I opened a new crontab where I schedule the bash script to run every morning at 7am."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Thoughts:\n",
    "\n",
    "Even though I successfully automated this process, I realize that some of my code here is, perhaps, not 'pythonic'. Speed was not a concern here. The point was to successfully get from point A to point B, which I did. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
